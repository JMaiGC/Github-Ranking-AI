[Github Ranking](../README.md)
==========

## Top 100 Stars in MoE

| Ranking | Project Name | Stars | Forks | Language | Open Issues | Description | Last Commit |
| ------- | ------------ | ----- | ----- | -------- | ----------- | ----------- | ----------- |
| 1 | [LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory) | 42932 | 5244 | Python | 327 | Unified Efficient Fine-Tuning of 100+ LLMs & VLMs (ACL 2024) | 2025-03-03T16:17:10Z |
| 2 | [sglang](https://github.com/sgl-project/sglang) | 11297 | 1134 | Python | 324 | SGLang is a fast serving framework for large language models and vision language models. | 2025-03-04T11:00:48Z |
| 3 | [trace.moe](https://github.com/soruly/trace.moe) | 4599 | 237 | None | 0 | Anime Scene Search by Image | 2024-10-13T03:00:58Z |
| 4 | [Moeditor](https://github.com/Moeditor/Moeditor) | 4132 | 275 | JavaScript | 106 | (discontinued) Your all-purpose markdown editor. | 2020-07-07T01:08:32Z |
| 5 | [Bangumi](https://github.com/czy0729/Bangumi) | 4055 | 140 | TypeScript | 14 | :electron: An unofficial https://bgm.tv ui first app client for Android and iOS, built with React Native. 一个无广告、以爱好为驱动、不以盈利为目的、专门做 ACG 的类似豆瓣的追番记录，bgm.tv 第三方客户端。为移动端重新设计，内置大量加强的网页端难以实现的功能，且提供了相当的自定义选项。 目前已适配 iOS / Android / WSA、mobile / 简单 pad、light / dark theme、移动端网页。 | 2025-03-03T13:14:45Z |
| 6 | [MoeGoe](https://github.com/CjangCjengh/MoeGoe) | 2370 | 249 | Python | 27 | Executable file for VITS inference | 2023-08-22T07:17:37Z |
| 7 | [Moe-Counter](https://github.com/journey-ad/Moe-Counter) | 2123 | 229 | JavaScript | 6 | Moe counter badge with multiple themes! - 多种风格可选的萌萌计数器 | 2025-02-06T06:16:00Z |
| 8 | [MoE-LLaVA](https://github.com/PKU-YuanGroup/MoE-LLaVA) | 2106 | 132 | Python | 62 | Mixture-of-Experts for Large Vision-Language Models | 2024-12-03T09:08:16Z |
| 9 | [fastmoe](https://github.com/laekov/fastmoe) | 1650 | 191 | Python | 25 | A fast MoE impl for PyTorch | 2025-02-10T06:04:33Z |
| 10 | [MoBA](https://github.com/MoonshotAI/MoBA) | 1595 | 85 | Python | 8 | MoBA: Mixture of Block Attention for Long-Context LLMs | 2025-02-22T15:39:23Z |
| 11 | [MoeKoeMusic](https://github.com/iAJue/MoeKoeMusic) | 1539 | 104 | Vue | 30 | 一款开源简洁高颜值的酷狗第三方客户端 An open-source, concise, and aesthetically pleasing third-party client for KuGou that supports  Windows / macOS / Linux :electron: | 2025-03-03T03:35:24Z |
| 12 | [DeepSeek-MoE](https://github.com/deepseek-ai/DeepSeek-MoE) | 1535 | 266 | Python | 16 | DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models | 2024-01-16T12:18:10Z |
| 13 | [hcaptcha-challenger](https://github.com/QIN2DIM/hcaptcha-challenger) | 1496 | 259 | Python | 36 | 🥂 Gracefully face hCaptcha challenge with MoE(ONNX) embedded solution. | 2024-04-20T11:15:09Z |
| 14 | [OpenMoE](https://github.com/XueFuzhao/OpenMoE) | 1466 | 76 | Python | 5 | A family of open-sourced Mixture-of-Experts (MoE) Large Language Models | 2024-03-08T15:08:26Z |
| 15 | [paimon-moe](https://github.com/MadeBaruna/paimon-moe) | 1432 | 274 | JavaScript | 267 | Your best Genshin Impact companion! Help you plan what to farm with ascension calculator and database. Also track your progress with todo and wish counter. | 2025-03-04T09:02:04Z |
| 16 | [MOE](https://github.com/Yelp/MOE) | 1311 | 139 | C++ | 170 | A global, black box optimization engine for real world metric optimization. | 2023-03-24T11:00:32Z |
| 17 | [mixture-of-experts](https://github.com/davidmrau/mixture-of-experts) | 1051 | 108 | Python | 5 | PyTorch Re-Implementation of "The Sparsely-Gated Mixture-of-Experts Layer" by Noam Shazeer et al. https://arxiv.org/abs/1701.06538 | 2024-04-19T08:22:39Z |
| 18 | [Aria](https://github.com/rhymes-ai/Aria) | 1007 | 85 | Jupyter Notebook | 30 | Codebase for Aria - an Open Multimodal Native MoE | 2025-01-22T03:25:37Z |
| 19 | [MoeTTS](https://github.com/luoyily/MoeTTS) | 984 | 78 | None | 0 | Speech synthesis model /inference GUI repo for galgame characters based on Tacotron2, Hifigan, VITS and Diff-svc | 2023-03-03T07:30:05Z |
| 20 | [llama-moe](https://github.com/pjlab-sys4nlp/llama-moe) | 926 | 53 | Python | 5 | ⛷️ LLaMA-MoE: Building Mixture-of-Experts from LLaMA with Continual Pre-training (EMNLP 2024) | 2024-12-06T04:47:07Z |
| 21 | [moemail](https://github.com/beilunyang/moemail) | 822 | 278 | TypeScript | 6 | 一个基于 NextJS + Cloudflare 技术栈构建的可爱临时邮箱服务🎉 | 2025-03-01T02:42:57Z |
| 22 | [moebius](https://github.com/blocktronics/moebius) | 796 | 45 | JavaScript | 38 | Modern ANSI & ASCII Art Editor | 2024-05-02T15:54:35Z |
| 23 | [Tutel](https://github.com/microsoft/Tutel) | 778 | 95 | Python | 42 | Tutel MoE: An Optimized Mixture-of-Experts Implementation | 2025-03-04T02:37:37Z |
| 24 | [Adan](https://github.com/sail-sg/Adan) | 774 | 64 | Python | 3 | Adan: Adaptive Nesterov Momentum Algorithm for Faster Optimizing Deep Models | 2024-07-02T18:26:36Z |
| 25 | [MixtralKit](https://github.com/open-compass/MixtralKit) | 766 | 80 | Python | 12 | A toolkit for inference and evaluation of 'mixtral-8x7b-32kseqlen' from Mistral AI | 2023-12-15T19:10:55Z |
| 26 | [moe-theme.el](https://github.com/kuanyui/moe-theme.el) | 757 | 63 | Emacs Lisp | 14 | A customizable colorful eye-candy theme for Emacser. Moe, moe, kyun! | 2025-02-03T18:08:33Z |
| 27 | [MoeMemosAndroid](https://github.com/mudkipme/MoeMemosAndroid) | 710 | 74 | Kotlin | 71 | An app to help you capture thoughts and ideas | 2025-02-15T07:11:32Z |
| 28 | [UMOE-Scaling-Unified-Multimodal-LLMs](https://github.com/HITsz-TMG/UMOE-Scaling-Unified-Multimodal-LLMs) | 695 | 40 | Python | 11 | The codes about "Uni-MoE: Scaling Unified Multimodal Models with Mixture of Experts" | 2025-01-27T13:40:11Z |
| 29 | [moe](https://github.com/fox0430/moe) | 673 | 33 | Nim | 85 | A command line based editor inspired by Vim. Written in Nim. | 2025-03-01T06:16:17Z |
| 30 | [vtbs.moe](https://github.com/dd-center/vtbs.moe) | 621 | 38 | Vue | 32 | Virtual YouTubers in bilibili | 2024-09-10T06:07:07Z |
| 31 | [moedict-webkit](https://github.com/g0v/moedict-webkit) | 615 | 99 | Objective-C | 102 | 萌典網站 | 2024-11-21T16:19:23Z |
| 32 | [satania.moe](https://github.com/Pizzacus/satania.moe) | 614 | 57 | HTML | 3 | Satania IS the BEST waifu, no really, she is, if you don't believe me, this website will convince you | 2022-10-09T23:19:01Z |
| 33 | [moebius](https://github.com/robconery/moebius) | 604 | 43 | Elixir | 3 | A functional query tool for Elixir | 2024-10-23T18:55:45Z |
| 34 | [SmartImage](https://github.com/Decimation/SmartImage) | 604 | 27 | C# | 7 | Reverse image search tool (SauceNao, IQDB, Ascii2D, trace.moe, and more) | 2025-02-25T19:05:59Z |
| 35 | [Chinese-Mixtral](https://github.com/ymcui/Chinese-Mixtral) | 598 | 44 | Python | 0 | 中文Mixtral混合专家大模型（Chinese Mixtral MoE LLMs） | 2024-04-30T04:29:06Z |
| 36 | [Awesome-Mixture-of-Experts-Papers](https://github.com/codecaution/Awesome-Mixture-of-Experts-Papers) | 590 | 44 | None | 1 | A curated reading list of research in Mixture-of-Experts(MoE). | 2024-10-30T07:48:14Z |
| 37 | [MoeGoe_GUI](https://github.com/CjangCjengh/MoeGoe_GUI) | 571 | 67 | C# | 8 | GUI for MoeGoe | 2023-08-22T07:32:08Z |
| 38 | [moebooru](https://github.com/moebooru/moebooru) | 545 | 81 | Ruby | 26 | Moebooru, a fork of danbooru1 that has been heavily modified | 2025-02-19T13:35:49Z |
| 39 | [MoeList](https://github.com/axiel7/MoeList) | 539 | 19 | Kotlin | 24 | Another unofficial Android MAL client | 2025-03-02T11:21:15Z |
| 40 | [trace.moe-telegram-bot](https://github.com/soruly/trace.moe-telegram-bot) | 528 | 82 | JavaScript | 0 | This Telegram Bot can tell the anime when you send an screenshot to it | 2025-03-03T13:07:44Z |
| 41 | [MoeMemos](https://github.com/mudkipme/MoeMemos) | 520 | 45 | Swift | 58 | An app to help you capture thoughts and ideas | 2025-02-18T01:33:32Z |
| 42 | [moepush](https://github.com/beilunyang/moepush) | 517 | 57 | TypeScript | 1 | 一个基于 NextJS + Cloudflare 技术栈构建的可爱消息推送服务, 支持多种消息推送渠道✨ | 2025-03-03T08:21:54Z |
| 43 | [Time-MoE](https://github.com/Time-MoE/Time-MoE) | 476 | 39 | Python | 10 | [ICLR 2025 Spotlight] Official implementation of "Time-MoE: Billion-Scale Time Series Foundation Models with Mixture of Experts" | 2025-02-26T13:58:42Z |
| 44 | [step_into_llm](https://github.com/mindspore-courses/step_into_llm) | 454 | 110 | Jupyter Notebook | 27 | MindSpore online courses: Step into LLM | 2025-01-06T01:50:19Z |
| 45 | [moerail](https://github.com/Arnie97/moerail) | 431 | 29 | JavaScript | 13 | 铁路车站代码查询 × 动车组交路查询 | 2023-02-27T03:37:18Z |
| 46 | [MOE](https://github.com/google/MOE) | 422 | 76 | Java | 18 | Make Opensource Easy - tools for synchronizing repositories | 2022-06-20T22:41:08Z |
| 47 | [hydra-moe](https://github.com/SkunkworksAI/hydra-moe) | 412 | 15 | Python | 10 | None | 2023-11-02T22:53:15Z |
| 48 | [pixiv.moe](https://github.com/kokororin/pixiv.moe) | 363 | 43 | TypeScript | 0 | 😘 A pinterest-style layout site, shows illusts on pixiv.net order by popularity. | 2023-03-08T06:54:34Z |
| 49 | [WThermostatBeca](https://github.com/fashberg/WThermostatBeca) | 353 | 70 | C++ | 3 | Open Source firmware replacement for Tuya Wifi Thermostate from Beca and Moes with Home Assistant Autodiscovery | 2023-08-26T22:10:38Z |
| 50 | [notify.moe](https://github.com/animenotifier/notify.moe) | 350 | 45 | Go | 86 | :dancer: Anime tracker, database and community. Moved to https://git.akyoto.dev/web/notify.moe | 2022-09-26T07:15:05Z |
| 51 | [MoeLoaderP](https://github.com/xplusky/MoeLoaderP) | 341 | 24 | C# | 11 | 🖼二次元图片下载器 Pics downloader for booru sites,Pixiv.net,Bilibili.com,Konachan.com,Yande.re , behoimi.org, safebooru, danbooru,Gelbooru,SankakuComplex,Kawainyan,MiniTokyo,e-shuushuu,Zerochan,WorldCosplay ,Yuriimg etc. | 2023-10-18T23:13:10Z |
| 52 | [MOEAFramework](https://github.com/MOEAFramework/MOEAFramework) | 333 | 128 | Java | 0 | A Free and Open Source Java Framework for Multiobjective Optimization | 2025-03-03T17:16:32Z |
| 53 | [dialogue.moe](https://github.com/windrises/dialogue.moe) | 319 | 8 | Python | 1 | None | 2022-12-14T14:50:38Z |
| 54 | [moe-sticker-bot](https://github.com/star-39/moe-sticker-bot) | 316 | 34 | Go | 25 | A Telegram bot that imports LINE/kakao stickers or creates/manages new sticker set. | 2024-06-06T15:28:28Z |
| 55 | [st-moe-pytorch](https://github.com/lucidrains/st-moe-pytorch) | 314 | 28 | Python | 5 | Implementation of ST-Moe, the latest incarnation of MoE after years of research at Brain, in Pytorch | 2024-06-17T00:48:47Z |
| 56 | [moell-blog](https://github.com/moell-peng/moell-blog) | 302 | 81 | PHP | 2 | 基于 Laravel 开发，支持 Markdown 语法的博客 | 2022-07-31T11:51:54Z |
| 57 | [moeSS](https://github.com/wzxjohn/moeSS) | 299 | 107 | PHP | 11 | moe SS Front End for https://github.com/mengskysama/shadowsocks/tree/manyuser | 2015-02-27T08:44:30Z |
| 58 | [DiT-MoE](https://github.com/feizc/DiT-MoE) | 287 | 13 | Python | 5 | Scaling Diffusion Transformers with Mixture of Experts | 2024-09-09T02:12:12Z |
| 59 | [moe](https://github.com/MoeOrganization/moe) | 279 | 46 | Scala | 18 | An -OFun prototype of an Ultra Modern Perl 5 | 2013-09-27T18:39:18Z |
| 60 | [Cornell-MOE](https://github.com/wujian16/Cornell-MOE) | 268 | 63 | C++ | 25 | A Python library for the state-of-the-art Bayesian optimization algorithms, with the core implemented in C++. | 2020-02-04T18:39:37Z |
| 61 | [soft-moe-pytorch](https://github.com/lucidrains/soft-moe-pytorch) | 265 | 8 | Python | 4 | Implementation of Soft MoE, proposed by Brain's Vision team, in Pytorch | 2024-04-24T15:23:45Z |
| 62 | [GRIN-MoE](https://github.com/microsoft/GRIN-MoE) | 261 | 17 | None | 0 | GRadient-INformed MoE | 2024-09-25T18:46:48Z |
| 63 | [android-app](https://github.com/LISTEN-moe/android-app) | 260 | 25 | Kotlin | 7 | Official LISTEN.moe Android app | 2025-03-02T14:44:12Z |
| 64 | [MoeSR](https://github.com/TeamMoeAI/MoeSR) | 254 | 8 | JavaScript | 7 | An application specialized in image super-resolution for ACGN illustrations and Visual Novel CG. 专注于插画/Galgame CG等ACGN领域的图像超分辨率的应用 | 2024-04-17T12:34:26Z |
| 65 | [MoeQuest](https://github.com/HotBitmapGG/MoeQuest) | 252 | 76 | Java | 1 | The meizi of a material design style welfare App. | 2017-02-14T14:13:53Z |
| 66 | [parameter-efficient-moe](https://github.com/for-ai/parameter-efficient-moe) | 251 | 16 | Python | 1 | None | 2023-10-31T19:21:15Z |
| 67 | [MoeLoader-Delta](https://github.com/usaginya/MoeLoader-Delta) | 241 | 37 | C# | 52 | Improved branching version of MoeLoader | 2021-07-22T20:47:41Z |
| 68 | [moeins](https://github.com/iAJue/moeins) | 240 | 68 | PHP | 2 | 萌音影视 - 在线影视应用 | 2018-10-31T01:47:27Z |
| 69 | [inferflow](https://github.com/inferflow/inferflow) | 237 | 25 | C++ | 8 | Inferflow is an efficient and highly configurable inference engine for large language models (LLMs). | 2024-03-15T06:52:33Z |
| 70 | [moebius](https://github.com/moebiusphp/moebius) | 231 | 4 | PHP | 4 | True coroutines for PHP>=8.1 without worrying about event loops and callbacks. | 2022-06-08T23:18:45Z |
| 71 | [gdx-pay](https://github.com/libgdx/gdx-pay) | 226 | 86 | Java | 8 | A libGDX cross-platform API for InApp purchasing. | 2025-01-02T19:28:20Z |
| 72 | [ModuleFormer](https://github.com/IBM/ModuleFormer) | 215 | 11 | Python | 2 | ModuleFormer is a MoE-based architecture that includes two different types of experts: stick-breaking attention heads and feedforward experts. We released a collection of ModuleFormer-based Language Models (MoLM) ranging in scale from 4 billion to 8 billion parameters. | 2024-04-10T18:16:32Z |
| 73 | [MoH](https://github.com/SkyworkAI/MoH) | 210 | 8 | Python | 1 | MoH: Multi-Head Attention as Mixture-of-Head Attention | 2024-10-29T15:22:54Z |
| 74 | [moe](https://github.com/facebookresearch/moe) | 203 | 22 | None | 1 | Misspelling Oblivious Word Embeddings | 2019-08-06T12:42:31Z |
| 75 | [fiddler](https://github.com/efeslab/fiddler) | 196 | 18 | Python | 2 | [ICLR'25] Fast Inference of MoE Models with CPU-GPU Orchestration | 2024-11-18T00:25:45Z |
| 76 | [MoE-Adapters4CL](https://github.com/JiazuoYu/MoE-Adapters4CL) | 190 | 13 | Python | 3 | Code for paper "Boosting Continual Learning of Vision-Language Models via Mixture-of-Experts Adapters" CVPR2024 | 2024-11-17T05:47:00Z |
| 77 | [MoePhoto](https://github.com/opteroncx/MoePhoto) | 188 | 23 | Python | 5 | MoePhoto Image Toolbox萌图工具箱 | 2024-09-23T06:35:27Z |
| 78 | [MoE-plus-plus](https://github.com/SkyworkAI/MoE-plus-plus) | 188 | 6 | Python | 0 | [ICLR 2025] MoE++: Accelerating Mixture-of-Experts Methods with Zero-Computation Experts | 2024-10-16T06:21:31Z |
| 79 | [Yuan2.0-M32](https://github.com/IEIT-Yuan/Yuan2.0-M32) | 184 | 42 | Python | 6 | Mixture-of-Experts (MoE) Language Model  | 2024-09-09T09:14:15Z |
| 80 | [PlanMoE](https://github.com/JiangkuoWang/PlanMoE) | 177 | 19 | Python | 5 | This is a repository aimed at accelerating the training of MoE models, offering a more efficient scheduling method. | 2025-02-14T01:39:37Z |
| 81 | [ghost-theme-Moegi](https://github.com/moegi-design/ghost-theme-Moegi) | 164 | 26 | Handlebars | 3 | An elegant & fresh ghost theme. | 2023-10-16T16:09:28Z |
| 82 | [MOEAD](https://github.com/425776024/MOEAD) | 162 | 42 | Python | 2 | MOEAD.多目标差分进化算法的学习，Python实现&动态展示过程 | 2022-06-22T02:07:23Z |
| 83 | [Frequency_Aug_VAE_MoESR](https://github.com/tencent-ailab/Frequency_Aug_VAE_MoESR) | 154 | 4 | Python | 12 | Latent-based SR using MoE and frequency augmented VAE decoder | 2023-11-26T10:33:36Z |
| 84 | [moedict-desktop](https://github.com/racklin/moedict-desktop) | 152 | 15 | C++ | 6 | MoeDict Desktop For MacOSX / Linux / Windows | 2016-10-14T06:49:17Z |
| 85 | [Teyvat.moe](https://github.com/EliteMasterEric/Teyvat.moe) | 150 | 44 | TypeScript | 102 | A flexible, community-driven interactive website for Genshin Impact. | 2021-08-02T01:43:13Z |
| 86 | [guide.encode.moe](https://github.com/Irrational-Encoding-Wizardry/guide.encode.moe) | 150 | 20 | Markdown | 9 | A guide for fansubbing | 2024-04-14T10:27:37Z |
| 87 | [MOELoRA-peft](https://github.com/liuqidong07/MOELoRA-peft) | 147 | 19 | Python | 4 | [SIGIR'24] The official implementation code of MOELoRA. | 2024-07-22T07:32:43Z |
| 88 | [moeda](https://github.com/thompsonemerson/moeda) | 143 | 21 | JavaScript | 5 | :moneybag: :chart_with_upwards_trend: A foreign exchange rates and currency conversion using CLI | 2023-06-25T15:30:33Z |
| 89 | [MoeMusic](https://github.com/cpacm/MoeMusic) | 142 | 52 | Java | 2 | 一款基于萌否网站api的音乐管理软件 | 2017-01-22T06:29:59Z |
| 90 | [MoEx](https://github.com/Boyiliee/MoEx) | 142 | 19 | Python | 1 | MoEx (Moment Exchange) | 2021-06-24T02:52:22Z |
| 91 | [makegirlsmoe.github.io](https://github.com/makegirlsmoe/makegirlsmoe.github.io) | 141 | 35 | CSS | 0 | MakeGirls.moe Official Blog | 2017-08-21T15:06:57Z |
| 92 | [Parameter-Efficient-MoE](https://github.com/wuhy68/Parameter-Efficient-MoE) | 140 | 18 | Python | 3 | Parameter-Efficient Sparsity Crafting From Dense to Mixture-of-Experts for Instruction Tuning on General Tasks | 2024-09-20T02:18:30Z |
| 93 | [MoE-Infinity](https://github.com/EfficientMoE/MoE-Infinity) | 140 | 12 | Python | 7 | PyTorch library for cost-effective, fast and easy serving of MoE models. | 2025-02-27T23:43:11Z |
| 94 | [awesome-adaptive-computation](https://github.com/koayon/awesome-adaptive-computation) | 138 | 9 | None | 0 | A curated reading list of research in Adaptive Computation, Inference-Time Computation & Mixture of Experts (MoE). | 2025-01-01T13:49:09Z |
| 95 | [moedict-data](https://github.com/g0v/moedict-data) | 137 | 28 | None | 0 | 教育部重編國語辭典 資料檔; 若有建議或 bug 請在 moedict-process 反應 | 2023-02-17T00:42:39Z |
| 96 | [nonebot-plugin-moegoe](https://github.com/Yiyuiii/nonebot-plugin-moegoe) | 136 | 15 | Python | 9 | 用API让原神角色说话！ | 2024-05-27T06:14:27Z |
| 97 | [theindex](https://github.com/Snaacky/theindex) | 134 | 27 | TypeScript | 1 | The frontend, editor panel, and API of TheIndex.moe | 2025-02-19T00:54:22Z |
| 98 | [MixLoRA](https://github.com/TUDB-Labs/MixLoRA) | 134 | 15 | Python | 1 | State-of-the-art Parameter-Efficient MoE Fine-tuning Method | 2024-08-22T08:02:04Z |
| 99 | [archbox](https://github.com/lemniskett/archbox) | 131 | 5 | Shell | 1 | Easy to use Arch Linux chroot environment with some functionalities to integrate it with your existing Linux installation. Mirror of https://momodev.lemniskett.moe/lemniskett/archbox | 2022-05-28T15:46:32Z |
| 100 | [Skywork-MoE](https://github.com/SkyworkAI/Skywork-MoE) | 128 | 8 | None | 5 | Skywork-MoE: A Deep Dive into Training Techniques for Mixture-of-Experts Language Models | 2024-06-12T03:42:15Z |

